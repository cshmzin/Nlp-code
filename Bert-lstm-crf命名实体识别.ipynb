{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert命名实体识别.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYrfK33jyCpnfrfEaDM3KT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cshmzin/nlp-code/blob/main/Bert-lstm-crf%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcpUI2PrcymW"
      },
      "source": [
        "# 获取数据\n",
        "import json\n",
        "import os\n",
        "\n",
        "train_data = []\n",
        "dev_data = []\n",
        "\n",
        "for line in open('sample_data/train.json','r',encoding='UTF-8'):\n",
        "    train_data.append(json.loads(line))\n",
        "\n",
        "for line in open('sample_data/dev.json','r',encoding='UTF-8'):\n",
        "    dev_data.append(json.loads(line))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnIBc7s5Pjvc"
      },
      "source": [
        "#上图为标签类别\n",
        "#需要构建标签\n",
        "import re\n",
        "\n",
        "label_type = {'o':0,'address':1,'company':2,'name':3,'organization':4,'pad': 5}\n",
        "\n",
        "def decode_label(d):\n",
        "#解析标签，以列表形式构成\n",
        "  text_len = len(d['text'])\n",
        "  label = [0]*text_len\n",
        "  types = d['label'].keys()\n",
        "  for t in types:\n",
        "    if t in label_type:\n",
        "      values = d['label'][t].values()\n",
        "      si = [v for value in values for v in value]\n",
        "      for i in si:\n",
        "        for j in range(i[0],i[1]+1):\n",
        "          label[j] = label_type[t]\n",
        "  return label\n",
        "\n",
        "\n",
        "\n",
        "def transfrom_data(data,mode):\n",
        "  data_texts = [d['text'] for d in data]\n",
        "  \n",
        "  if mode == 'train':\n",
        "    data_labels = []\n",
        "    for d in data:\n",
        "      data_labels.append(decode_label(d))\n",
        "    return (data_texts,data_labels)\n",
        "  \n",
        "  else:\n",
        "    return data_texts \n",
        "\n",
        "train_texts,train_labels = transfrom_data(train_data,'train')\n",
        "dev_texts,dev_labels = transfrom_data(dev_data,'train')\n",
        "test_texts = transfrom_data(train_data,'test')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTqHOaS71FJ7"
      },
      "source": [
        "! pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# 使用bert的tokenizer将文字转化成数字。\n",
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定为中文\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "clear_output()\n",
        "\n",
        "train_ids = []\n",
        "dev_ids = []\n",
        "\n",
        "tokens = [[tokenizer.tokenize(t)[0] for t in text] for text in train_texts]\n",
        "train_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "\n",
        "tokens = [[tokenizer.tokenize(t)[0] for t in text] for text in dev_texts]\n",
        "dev_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "\n",
        "dev_labels = [label for label in dev_labels]\n",
        "train_labels = [label for label in train_labels]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKc5_CRu5XtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573f9feb-ca86-4a37-d54d-e091a4863e32"
      },
      "source": [
        "import torch\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from transformers import BertTokenizer\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class Dataset():\n",
        "    def __init__(self):\n",
        "        self.label_type = {'o': 0, 'address': 1, 'company': 2, 'name': 3, 'organization': 4, 'pad': 5}\n",
        "        clear_output()\n",
        "\n",
        "    def pad(self,ids,labels):\n",
        "\n",
        "        input_ids = pad_sequences(ids,maxlen=60,dtype='long', value=0.0,truncating=\"post\", padding=\"post\")\n",
        "        tags = pad_sequences(labels,maxlen=60, value=self.label_type[\"pad\"], padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
        "        attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
        "        return (input_ids,tags,attention_masks)\n",
        "\n",
        "    def loader(self,ids,labels):\n",
        "        input_ids,tags,attention_masks = self.pad(ids,labels)\n",
        "        dataset = TensorDataset(torch.tensor(input_ids),torch.tensor(tags),torch.tensor(attention_masks))\n",
        "        dataloader = DataLoader(dataset,batch_size=64)\n",
        "        print('dataloader load ok')\n",
        "        return dataloader\n",
        "\n",
        "dataloaders = Dataset()\n",
        "trainloader = dataloaders.loader(train_ids,train_labels)\n",
        "devloader = dataloaders.loader(dev_ids,dev_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataloader load ok\n",
            "dataloader load ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wi1oSxEdT9f",
        "outputId": "839bd888-b629-4a75-9709-13a4e6d46830"
      },
      "source": [
        "! pip install pytorch-crf\r\n",
        "from transformers import BertPreTrainedModel,BertModel\r\n",
        "from torchcrf import CRF\r\n",
        "import torch.nn as nn\r\n",
        "class BertLstmCrf(BertPreTrainedModel):\r\n",
        "\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\r\n",
        "\r\n",
        "    def __init__(self, config,need_bilstm = False,rnn_dim = 128):\r\n",
        "        super().__init__(config)\r\n",
        "        self.num_labels = config.num_labels\r\n",
        "\r\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\r\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.out_dim = config.hidden_size\r\n",
        "        self.need_bilstm = need_bilstm\r\n",
        "        if need_bilstm:\r\n",
        "            self.bilstm = nn.LSTM(config.hidden_size, rnn_dim, num_layers=1, bidirectional=True, batch_first=True)\r\n",
        "            self.out_dim = 2*rnn_dim\r\n",
        "        self.liner = nn.Linear(self.out_dim, config.num_labels)\r\n",
        "        self.crf = CRF(config.num_labels,batch_first=True)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None,labels=None,):\r\n",
        "\r\n",
        "        outputs = self.bert(\r\n",
        "            input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "        )\r\n",
        "\r\n",
        "        sequence_output = outputs[0]\r\n",
        "        if self.need_bilstm:\r\n",
        "            sequence_output,_ = self.bilstm(sequence_output)\r\n",
        "        sequence_output = self.dropout(sequence_output)\r\n",
        "        sequence_output = self.liner(sequence_output)\r\n",
        "        loss = -1 * self.crf(sequence_output, labels, mask=attention_mask.byte()) if labels != None else None\r\n",
        "        output = self.crf.decode(sequence_output, attention_mask.byte())\r\n",
        "\r\n",
        "        return [loss,output] if loss is not None else output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.7/dist-packages (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5WKjbooRC3I",
        "outputId": "50b178ac-1629-4b48-e744-48d1e162b229"
      },
      "source": [
        "! pip install seqeval\n",
        "from transformers import BertForTokenClassification\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from seqeval.metrics import f1_score as f1\n",
        "import os\n",
        "\n",
        "model = BertLstmCrf.from_pretrained(\"bert-base-chinese\", num_labels=6)\n",
        "need_CRF = True\n",
        "# model =  BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_labels=6)\n",
        "# need_CRF = False\n",
        "model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
        "Epochs = 10\n",
        "type_label = {0:'o',1:'address',2:'company',3:'name',4:'organization',5:'pad'}\n",
        "\n",
        "if os.path.exists('sample_data/bert_lstm_crf.pth'):model.load_state_dict(torch.load('sample_data/bert_lstm_crf.pth'))\n",
        "\n",
        "for epoch in range(Epochs):\n",
        "  model.train()\n",
        "  losses = 0.0\n",
        "  for data in trainloader:\n",
        "      tokens_tensors,label_tensors,masks_tensors = [t.cuda() for t in data]\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(input_ids = tokens_tensors,attention_mask = masks_tensors,labels = label_tensors)\n",
        "      loss = outputs[0]\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses += loss.item()\n",
        "  avg_train_loss = losses / len(trainloader)\n",
        "  print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "  \n",
        "  model.eval()\n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "\n",
        "  if not need_CRF:\n",
        "    for data in devloader:\n",
        "      tokens_tensors, label_tensors, masks_tensors = [t.cuda() for t in data]\n",
        "      with torch.no_grad():\n",
        "        preds = model(input_ids=tokens_tensors, attention_mask=masks_tensors)\n",
        "      \n",
        "      for pred,label_tensor in zip(preds[0],label_tensors):\n",
        "        logit = pred.detach().cpu().numpy()#detach的方法，将variable参数从网络中隔离开，不参与参数更新\n",
        "        label_ids = label_tensor.cpu().numpy()\n",
        "\n",
        "        predictions.extend(np.argmax(logit, axis=1))\n",
        "        true_labels.append(label_ids)\n",
        "\n",
        "    pred_tags = list(np.array(predictions).flatten())\n",
        "    valid_tags = list(np.array(true_labels).flatten())\n",
        "    print(\"F1-Score: {}\".format(f1_score(pred_tags,valid_tags,average='weighted')))#传入的是具体的tag\n",
        "\n",
        "  else:\n",
        "    for batch in devloader:\n",
        "      tokens_tensors, label_tensors, masks_tensors = [t.cuda() for t in data]\n",
        "      with torch.no_grad():\n",
        "        outputs = model(input_ids=tokens_tensors, attention_mask=masks_tensors,labels=label_tensors)\n",
        "      logits = outputs[1]\n",
        "      label_ids = label_tensors.cpu().numpy()\n",
        "\n",
        "      predictions.extend(logits)\n",
        "      true_labels.extend(list(label_ids))\n",
        "\n",
        "    pred_tags = [[type_label[p_i] for p, l in zip(predictions, true_labels)\n",
        "                  for p_i, l_i in zip(p, l) if type_label[l_i] != \"pad\"]]\n",
        "    valid_tags = [[type_label[l_i] for l in true_labels\n",
        "                    for l_i in l if type_label[l_i] != \"pad\"]]\n",
        "    print(\"Validation F1-Score: {}\".format(f1(pred_tags, valid_tags)))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'sample_data/bert_lstm_crf.pth')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertLstmCrf: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertLstmCrf from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLstmCrf from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLstmCrf were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['liner.weight', 'liner.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 28.61979950041998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: o seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: organization seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: address seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: company seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: name seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation F1-Score: 0.9738562091503268\n",
            "Average train loss: 25.63988749186198\n",
            "Validation F1-Score: 0.9738562091503268\n",
            "Average train loss: 23.452751988456363\n",
            "Validation F1-Score: 0.974025974025974\n",
            "Average train loss: 26.00460913067772\n",
            "Validation F1-Score: 0.9607843137254902\n",
            "Average train loss: 21.062477208319166\n",
            "Validation F1-Score: 0.961038961038961\n",
            "Average train loss: 19.2672103927249\n",
            "Validation F1-Score: 0.974025974025974\n",
            "Average train loss: 20.661885148002987\n",
            "Validation F1-Score: 0.974025974025974\n",
            "Average train loss: 16.21223018850599\n",
            "Validation F1-Score: 0.9738562091503268\n",
            "Average train loss: 15.863000744865055\n",
            "Validation F1-Score: 0.974025974025974\n",
            "Average train loss: 14.482931227911086\n",
            "Validation F1-Score: 0.974025974025974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "xzAgjTa33_Mi",
        "outputId": "fc9d18f7-15e2-4da6-a327-ab959ffb0b8c"
      },
      "source": [
        "text = '360集团发布一个漏洞'\n",
        "need_CRF = True\n",
        "test_tokens = tokenizer.tokenize(text)\n",
        "test_ids = tokenizer.convert_tokens_to_ids(test_tokens)\n",
        "test_tokens_tensor = torch.tensor(test_ids)\n",
        "test_tokens_tensor = test_tokens_tensor\n",
        "\n",
        "test_masks_tensor = torch.zeros(test_tokens_tensor.shape, dtype=torch.long)\n",
        "test_masks_tensor = test_masks_tensor.masked_fill(test_tokens_tensor != 0, 1)\n",
        "\n",
        "if not need_CRF:\n",
        "  outputs = model(input_ids=test_tokens_tensor.unsqueeze(0).cuda(),attention_mask=test_masks_tensor.unsqueeze(0).cuda())\n",
        "  logits = outputs[0]\n",
        "  preds = []\n",
        "  for logit in logits:\n",
        "    preds.extend(np.argmax(logit.detach().cpu().numpy(), axis=1))\n",
        "\n",
        "  inverse_dict=dict([val,key] for key,val in label_type.items())\n",
        "  preds = [inverse_dict[i] for i in preds]\n",
        "\n",
        "  print(test_tokens)\n",
        "  print(preds)\n",
        "\n",
        "else:\n",
        "  logits = model(input_ids=test_tokens_tensor.unsqueeze(0).cuda(),attention_mask=test_masks_tensor.unsqueeze(0).cuda())[0]\n",
        "\n",
        "  preds = [type_label[i] for i in logits]\n",
        "\n",
        "  print(test_tokens)\n",
        "  print(preds)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-072aa08641e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'360集团发布一个漏洞'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mCRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_tokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    }
  ]
}