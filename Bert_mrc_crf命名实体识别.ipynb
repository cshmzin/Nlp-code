{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNv8i67jMLL+gTxv1Pw5UdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cshmzin/nlp-code/blob/main/Bert_mrc_crf%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCBm7bXwrQeM"
      },
      "source": [
        "# 获取数据\r\n",
        "import json\r\n",
        "import os\r\n",
        "\r\n",
        "train_data = []\r\n",
        "dev_data = []\r\n",
        "\r\n",
        "for line in open('sample_data/train.json','r',encoding='UTF-8'):\r\n",
        "    train_data.append(json.loads(line))\r\n",
        "\r\n",
        "for line in open('sample_data/dev.json','r',encoding='UTF-8'):\r\n",
        "    dev_data.append(json.loads(line))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72TGHE1xr-94",
        "outputId": "930c2f7e-f3e4-47df-d806-a2b56cfbb923"
      },
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': '浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，', 'label': {'name': {'叶老桂': [[9, 11]]}, 'company': {'浙商银行': [[0, 3]]}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piqg3v27sHmg"
      },
      "source": [
        "#上图为标签类别\r\n",
        "#需要构建标签\r\n",
        "import re\r\n",
        "\r\n",
        "label_type = {'o':0,'address':1,'company':2,'name':3,'organization':4,'pad': 5}\r\n",
        "label_meaning = {'address':'找到某人或某机关或与其通信的指定地点','company':'企业的组织形式,以营利为目的的社团法人','name':'人或者产品、物体的名称','organization':'人们为实现一定的目标，互相协作结合而成的集体或团体'}\r\n",
        "label_all = ['address','company','name','organization']\r\n",
        "\r\n",
        "def decode_label(d,Type):\r\n",
        "#解析标签，以列表形式构成\r\n",
        "  text_len = len(d['text'])\r\n",
        "  label = [0]*text_len\r\n",
        "  types = d['label'].keys()\r\n",
        "  for t in types:\r\n",
        "    if t == Type:\r\n",
        "      values = d['label'][t].values()\r\n",
        "      si = [v for value in values for v in value]\r\n",
        "      for i in si:\r\n",
        "        for j in range(i[0],i[1]+1):\r\n",
        "          label[j] = label_type[t]\r\n",
        "  return (d['text'],label_meaning[Type],label)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def transfrom_data(data,mode):\r\n",
        "  datas = []\r\n",
        "  if mode == 'train':\r\n",
        "    data_labels = []\r\n",
        "    for d in data:\r\n",
        "      for Type in label_all:\r\n",
        "        datas.append(decode_label(d,Type))\r\n",
        "    return datas\r\n",
        "  \r\n",
        "  else:\r\n",
        "    data_texts = [d['text'] for d in data]\r\n",
        "    return data_texts \r\n",
        "\r\n",
        "train_datas = transfrom_data(train_data,'train')\r\n",
        "dev_datas = transfrom_data(dev_data,'train')\r\n",
        "test_datas = transfrom_data(train_data,'test')\r\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTxjH6OBsNYA",
        "outputId": "0dc44da4-b934-4c78-90f3-e0b8eed44598"
      },
      "source": [
        "print(train_datas[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，', '找到某人或某机关或与其通信的指定地点', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjqX_00psSoY"
      },
      "source": [
        "! pip install transformers\r\n",
        "from transformers import BertTokenizer\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "# 使用bert的tokenizer将文字转化成数字。\r\n",
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定为中文\r\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\r\n",
        "clear_output()\r\n",
        "\r\n",
        "train_ids = []\r\n",
        "dev_ids = []\r\n",
        "\r\n",
        "tokens = [['CLS'] + [tokenizer.tokenize(t)[0] for t in data[1]] + ['SEP'] + [tokenizer.tokenize(t)[0] for t in data[0]] for data in train_datas]\r\n",
        "train_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\r\n",
        "\r\n",
        "dev_tokens = [['CLS'] + [tokenizer.tokenize(t)[0] for t in data[1]] + ['SEP'] + [tokenizer.tokenize(t)[0] for t in data[0]] for data in dev_datas]\r\n",
        "dev_ids = [tokenizer.convert_tokens_to_ids(token) for token in dev_tokens]\r\n",
        "\r\n",
        "dev_labels = [[0]*(len(id)-len(data[2]))+data[2] for id,data in zip(dev_ids,dev_datas)]\r\n",
        "train_labels = [[0]*(len(id)-len(data[2]))+data[2] for id,data in zip(train_ids,train_datas)]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLEeaIIxBTBL",
        "outputId": "6b4e2d3e-de63-4c3c-8172-e150727b4e3d"
      },
      "source": [
        "print(dev_tokens[0])\r\n",
        "print(dev_labels[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CLS', '彭', '小', '军', '认', '为', '，', '国', '内', '银', '行', '现', '在', '走', '的', '是', '台', '湾', '的', '发', '卡', '模', '式', '，', '先', '通', '过', '跑', '马', '圈', '地', '再', '在', '圈', '的', '地', '里', '面', '选', '择', '客', '户', '，', 'SEP', '找', '到', '某', '人', '或', '某', '机', '关', '或', '与', '其', '通', '信', '的', '指', '定', '地', '点']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP1fE_SXCVWa",
        "outputId": "888de608-0ef9-4056-eacc-7cd4da958e30"
      },
      "source": [
        "import torch\r\n",
        "from keras_preprocessing.sequence import pad_sequences\r\n",
        "from torch.utils.data import TensorDataset,DataLoader\r\n",
        "from transformers import BertTokenizer\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "class Dataset():\r\n",
        "    def __init__(self):\r\n",
        "        self.label_type = {'o': 0, 'address': 1, 'company': 2, 'name': 3, 'organization': 4, 'pad': 5}\r\n",
        "        clear_output()\r\n",
        "\r\n",
        "    def pad(self,ids,labels):\r\n",
        "\r\n",
        "        input_ids = pad_sequences(ids,maxlen=80,dtype='long', value=0.0,truncating=\"post\", padding=\"post\")\r\n",
        "        tags = pad_sequences(labels,maxlen=80, value=self.label_type[\"pad\"], padding=\"post\",dtype=\"long\", truncating=\"post\")\r\n",
        "        attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\r\n",
        "        position_ids = []\r\n",
        "        for ii in input_ids:\r\n",
        "          position = [1]*80\r\n",
        "          for num in range(80):\r\n",
        "            if num!=0 and ii[num] == 100:\r\n",
        "              break\r\n",
        "          for i in range(num+1):\r\n",
        "            position[i] = 0\r\n",
        "          position_ids.append(position)\r\n",
        "        return (input_ids,tags,attention_masks,position_ids)\r\n",
        "\r\n",
        "    def loader(self,ids,labels):\r\n",
        "        input_ids,tags,attention_masks,token_type_ids = self.pad(ids,labels)\r\n",
        "        dataset = TensorDataset(torch.tensor(input_ids),torch.tensor(tags),torch.tensor(attention_masks),torch.tensor(token_type_ids))\r\n",
        "        dataloader = DataLoader(dataset,batch_size=64)\r\n",
        "        print('dataloader load ok')\r\n",
        "        return dataloader\r\n",
        "\r\n",
        "dataloaders = Dataset()\r\n",
        "trainloader = dataloaders.loader(train_ids,train_labels)\r\n",
        "devloader = dataloaders.loader(dev_ids,dev_labels)\r\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataloader load ok\n",
            "dataloader load ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1z8m5j_YDhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "505499ed-1c29-445a-f363-06a77ee95772"
      },
      "source": [
        "! pip install pytorch-crf\r\n",
        "from transformers import BertPreTrainedModel,BertModel\r\n",
        "from torchcrf import CRF\r\n",
        "import torch.nn as nn\r\n",
        "class BertLstmCrf(BertPreTrainedModel):\r\n",
        "\r\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\r\n",
        "\r\n",
        "    def __init__(self, config,need_bilstm = False,rnn_dim = 128):\r\n",
        "        super().__init__(config)\r\n",
        "        self.num_labels = config.num_labels\r\n",
        "\r\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\r\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.out_dim = config.hidden_size\r\n",
        "        self.need_bilstm = need_bilstm\r\n",
        "        if need_bilstm:\r\n",
        "            self.bilstm = nn.LSTM(config.hidden_size, rnn_dim, num_layers=1, bidirectional=True, batch_first=True)\r\n",
        "            self.out_dim = 2*rnn_dim\r\n",
        "        self.liner = nn.Linear(self.out_dim, config.num_labels)\r\n",
        "        self.crf = CRF(config.num_labels,batch_first=True)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None,labels=None,):\r\n",
        "\r\n",
        "        outputs = self.bert(\r\n",
        "            input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids,\r\n",
        "        )\r\n",
        "\r\n",
        "        sequence_output = outputs[0]\r\n",
        "        if self.need_bilstm:\r\n",
        "            sequence_output,_ = self.bilstm(sequence_output)\r\n",
        "        sequence_output = self.dropout(sequence_output)\r\n",
        "        sequence_output = self.liner(sequence_output)\r\n",
        "        loss = -1 * self.crf(sequence_output, labels, mask=attention_mask.byte()) if labels != None else None\r\n",
        "        output = self.crf.decode(sequence_output, attention_mask.byte())\r\n",
        "\r\n",
        "        return [loss,output] if loss is not None else output"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.7/dist-packages (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nplj8Kr6_Gw2",
        "outputId": "9daad0dd-d4fe-46b0-8db1-7a21d731c7de"
      },
      "source": [
        "! pip install seqeval\r\n",
        "from transformers import BertForTokenClassification\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from seqeval.metrics import f1_score as f1\r\n",
        "import os\r\n",
        "\r\n",
        "model = BertLstmCrf.from_pretrained(\"bert-base-chinese\", num_labels=6)\r\n",
        "need_CRF = True\r\n",
        "# model =  BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_labels=6)\r\n",
        "# need_CRF = False\r\n",
        "model.cuda()\r\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\r\n",
        "Epochs = 6\r\n",
        "type_label = {0:'o',1:'address',2:'company',3:'name',4:'organization',5:'pad'}\r\n",
        "\r\n",
        "if os.path.exists('sample_data/bert_lstm_crf.pth'):model.load_state_dict(torch.load('sample_data/bert_lstm_crf.pth'))\r\n",
        "clear_output()\r\n",
        "for epoch in range(Epochs):\r\n",
        "  model.train()\r\n",
        "  losses = 0.0\r\n",
        "  for data in trainloader:\r\n",
        "      tokens_tensors,label_tensors,masks_tensors,token_type_ids = [t.cuda() for t in data]\r\n",
        "      optimizer.zero_grad()\r\n",
        "      outputs = model(input_ids = tokens_tensors,attention_mask = masks_tensors,token_type_ids = token_type_ids,labels = label_tensors)\r\n",
        "      loss = outputs[0]\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      losses += loss.item()\r\n",
        "  avg_train_loss = losses / len(trainloader)\r\n",
        "  print(\"Average train loss: {}\".format(avg_train_loss))\r\n",
        "  \r\n",
        "  model.eval()\r\n",
        "  predictions , true_labels = [], []\r\n",
        "\r\n",
        "\r\n",
        "  if not need_CRF:\r\n",
        "    for data in devloader:\r\n",
        "      tokens_tensors, label_tensors, masks_tensors,token_type_ids = [t.cuda() for t in data]\r\n",
        "      with torch.no_grad():\r\n",
        "        preds = model(input_ids=tokens_tensors, attention_mask=masks_tensors,token_type_ids=token_type_ids)\r\n",
        "      \r\n",
        "      for pred,label_tensor in zip(preds[0],label_tensors):\r\n",
        "        logit = pred.detach().cpu().numpy()#detach的方法，将variable参数从网络中隔离开，不参与参数更新\r\n",
        "        label_ids = label_tensor.cpu().numpy()\r\n",
        "\r\n",
        "        predictions.extend(np.argmax(logit, axis=1))\r\n",
        "        true_labels.append(label_ids)\r\n",
        "\r\n",
        "    pred_tags = list(np.array(predictions).flatten())\r\n",
        "    valid_tags = list(np.array(true_labels).flatten())\r\n",
        "    print(\"F1-Score: {}\".format(f1_score(pred_tags,valid_tags,average='weighted')))#传入的是具体的tag\r\n",
        "\r\n",
        "  else:\r\n",
        "    for batch in devloader:\r\n",
        "      tokens_tensors, label_tensors, masks_tensors,token_type_ids = [t.cuda() for t in batch]\r\n",
        "      with torch.no_grad():\r\n",
        "        outputs = model(input_ids=tokens_tensors, attention_mask=masks_tensors,token_type_ids=token_type_ids,labels=label_tensors)\r\n",
        "      logits = outputs[1]\r\n",
        "      label_ids = label_tensors.cpu().numpy()\r\n",
        "\r\n",
        "      predictions.extend(logits)\r\n",
        "      true_labels.extend(list(label_ids))\r\n",
        "\r\n",
        "    pred_tags = [[type_label[p_i] for p, l in zip(predictions, true_labels)\r\n",
        "                  for p_i, l_i in zip(p, l) if type_label[l_i] != \"pad\"]]\r\n",
        "    valid_tags = [[type_label[l_i] for l in true_labels\r\n",
        "                    for l_i in l if type_label[l_i] != \"pad\"]]\r\n",
        "    print(\"Validation F1-Score: {}\".format(f1(pred_tags, valid_tags)))\r\n",
        "\r\n",
        "\r\n",
        "torch.save(model.state_dict(), 'sample_data/bert_lstm_crf.pth')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 26.719476041339693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: o seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: organization seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: company seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: name seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: address seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "DSswuckeCDad",
        "outputId": "fa567f4f-e073-4949-d6b0-4ed66eff6a4f"
      },
      "source": [
        "text = '360集团发布一个漏洞'\r\n",
        "question = '企业的组织形式,以营利为目的的社团法人'\r\n",
        "\r\n",
        "need_CRF = True\r\n",
        "test_tokens = tokenizer.tokenize(text)\r\n",
        "question_tokens = tokenizer.tokenize(question)\r\n",
        "\r\n",
        "test_tokens = ['CLS'] + question_tokens + ['SEP'] + test_tokens\r\n",
        "test_ids = tokenizer.convert_tokens_to_ids(test_tokens)\r\n",
        "\r\n",
        "position_ids = [0]*(len(question_tokens)+2)+[1]*len(test_tokens)\r\n",
        "test_tokens_tensor = torch.tensor(test_ids)\r\n",
        "position_ids_tensor = torch.tensor(position_ids)\r\n",
        "test_masks_tensor = torch.zeros(test_tokens_tensor.shape, dtype=torch.long)\r\n",
        "test_masks_tensor = test_masks_tensor.masked_fill(test_tokens_tensor != 0, 1)\r\n",
        "\r\n",
        "if not need_CRF:\r\n",
        "  outputs = model(input_ids=test_tokens_tensor.unsqueeze(0).cuda(),attention_mask=test_masks_tensor.unsqueeze(0).cuda(),position_ids = position_ids_tensor.unsqueeze(0).cuda())\r\n",
        "  logits = outputs[0]\r\n",
        "  preds = []\r\n",
        "  for logit in logits:\r\n",
        "    preds.extend(np.argmax(logit.detach().cpu().numpy(), axis=1))\r\n",
        "\r\n",
        "  inverse_dict=dict([val,key] for key,val in label_type.items())\r\n",
        "  preds = [inverse_dict[i] for i in preds]\r\n",
        "\r\n",
        "  print(test_tokens)\r\n",
        "  print(preds)\r\n",
        "\r\n",
        "else:\r\n",
        "  logits = model(input_ids=test_tokens_tensor.unsqueeze(0).cuda(),attention_mask=test_masks_tensor.unsqueeze(0).cuda())[0]\r\n",
        "\r\n",
        "  preds = [type_label[i] for i in logits]\r\n",
        "\r\n",
        "  print(logits)\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f808a459fce6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mneed_CRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mquestion_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    }
  ]
}
